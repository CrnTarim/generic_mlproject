import sys
from dataclasses import dataclass
import os
import numpy as np 
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler

from src.exception import CustomException
from src.logger import logging
from src.utils import save_object

from src.components.data_ingestion import DataIngestion


# KonfigÃ¼rasyon: Preprocessor dosyasÄ±nÄ±n kaydedileceÄŸi yol
@dataclass
class DataTransformationConfig:
    preprocessor_obj_file_path = os.path.join('artifacts', "preprocessor.pkl")

class DataTransformation:
    def __init__(self):
        self.data_transformation_config = DataTransformationConfig()

    # Veriyi dÃ¶nÃ¼ÅŸtÃ¼recek preprocessor objesini oluÅŸtur
    def get_data_transformer_object(self):
        try:
            # SayÄ±sal ve kategorik sÃ¼tunlarÄ±n manuel listesi
            numerical_columns = ["writing_score", "reading_score"]
            categorical_columns = [
                "gender",
                "race_ethnicity",
                "parental_level_of_education",
                "lunch",
                "test_preparation_course",
            ]

            logging.info(f"Categorical columns: {categorical_columns}")
            logging.info(f"Numerical columns: {numerical_columns}")

            # SayÄ±sal veriler iÃ§in pipeline: eksik deÄŸer -> scale
            num_pipeline = Pipeline(
                steps=[
                    ("imputer", SimpleImputer(strategy="median")),
                    ("scaler", StandardScaler())
                ]
            )

            # Kategorik veriler iÃ§in pipeline: eksik deÄŸer -> OneHot -> scale (sparse yapÄ±da mean alÄ±nmaz)
            cat_pipeline = Pipeline(
                steps=[
                    ("imputer", SimpleImputer(strategy="most_frequent")),
                    ("one_hot_encoder", OneHotEncoder()),
                    ("scaler", StandardScaler(with_mean=False))  # OneHot sonrasÄ± sparse yapÄ± iÃ§in
                ]
            )

            # Her iki pipelineâ€™Ä± birleÅŸtir
            preprocessor = ColumnTransformer(
                transformers=[
                    ("num_pipeline", num_pipeline, numerical_columns),
                    ("cat_pipeline", cat_pipeline, categorical_columns)
                ]
            )

            return preprocessor

        except Exception as e:
            raise CustomException(e, sys)

    # EÄŸitim ve test verisi Ã¼zerinde preprocessing uygula
    def initiate_data_transformation(self, train_path, test_path):
        try:
            # Verileri oku
            train_df = pd.read_csv(train_path)
            test_df = pd.read_csv(test_path)

            logging.info("Train and test data read successfully")
            logging.info("Obtaining preprocessing object")

            preprocessing_obj = self.get_data_transformer_object()

            target_column_name = "math_score"
            numerical_columns = ["writing_score", "reading_score"]

            # Girdi (X) ve hedef (y) ayrÄ±mÄ±
            input_feature_train_df = train_df.drop(columns=[target_column_name], axis=1)
            target_feature_train_df = train_df[target_column_name]

            input_feature_test_df = test_df.drop(columns=[target_column_name], axis=1)
            target_feature_test_df = test_df[target_column_name]

            logging.info("Applying preprocessing on training and testing data")

            # Preprocessing uygula (fit + transform)
            input_feature_train_arr = preprocessing_obj.fit_transform(input_feature_train_df)
            input_feature_test_arr = preprocessing_obj.transform(input_feature_test_df)

            # Girdi ile hedefi tekrar birleÅŸtir (model eÄŸitimi iÃ§in)
            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train_df)]
            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]

            logging.info("Saving preprocessing object")

            # Preprocessor objesini dosyaya kaydet (ileride tekrar kullanÄ±lmak Ã¼zere)
            save_object(
                file_path=self.data_transformation_config.preprocessor_obj_file_path,
                obj=preprocessing_obj
            )

            # EÄŸitim ve test verisi + kaydedilen objenin yolu return edilir
            return (
                train_arr,
                test_arr,
                self.data_transformation_config.preprocessor_obj_file_path,
            )

        except Exception as e:
            raise CustomException(e, sys)


if __name__ == "__main__":
    # 1. Veri al
    ingestion = DataIngestion()
    train_path, test_path = ingestion.initiate_data_ingestion()

    # 2. DÃ¶nÃ¼ÅŸtÃ¼r
    transformer = DataTransformation()
    train_arr, test_arr, obj_path = transformer.initiate_data_transformation(train_path, test_path)

    print("âœ… Train verisi ÅŸekli:", train_arr.shape)
    print("âœ… Test verisi ÅŸekli:", test_arr.shape)
    print("âœ… Preprocessor kaydedildi:", obj_path)







#manuel inputu deÄŸiÅŸtir
#dÄ± ile yapmaya calÄ±s


# 1ï¸âƒ£ Preprocessor dosyasÄ±nÄ±n yolunu tanÄ±mlar (artifacts/preprocessor.pkl)
# 2ï¸âƒ£ SayÄ±sal ve kategorik sÃ¼tunlarÄ± belirler
# 3ï¸âƒ£ Her sÃ¼tun tipi iÃ§in ayrÄ± Pipeline oluÅŸturur
# 4ï¸âƒ£ SayÄ±sal verileri: impute (median) â†’ scale
# 5ï¸âƒ£ Kategorik verileri: impute (most_frequent) â†’ one-hot â†’ scale (with_mean=False)
# 6ï¸âƒ£ TÃ¼m pipelineâ€™larÄ± ColumnTransformer ile birleÅŸtirir
# 7ï¸âƒ£ Train ve test CSVâ€™lerini okur
# 8ï¸âƒ£ math_score hedef sÃ¼tununu ayÄ±rÄ±r
# 9ï¸âƒ£ Preprocessor objesini fit_transform ve transform ile uygular
# ğŸ”Ÿ Ä°ÅŸlenmiÅŸ X ile yâ€™yi birleÅŸtirir (np.c_[])
# 1ï¸âƒ£1ï¸âƒ£ Preprocessor objesini .pkl dosyasÄ± olarak kaydeder
# 1ï¸âƒ£2ï¸âƒ£ Train, test ve obje yolunu return eder
